{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Corriger les incompatibilitÃ©s de versions\n",
        "!pip install --upgrade --force-reinstall numpy==1.23.5 gensim==4.3.0 scipy==1.10.1 --quiet\n",
        "\n",
        "# RedÃ©marrer automatiquement l'environnement\n",
        "import os\n",
        "os.kill(os.getpid(), 9)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2yLc5Lx2Lvo",
        "outputId": "a56a1838-dde7-4939-a302-be2e0c4991b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/58.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.9/58.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m34.1/34.1 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.7/61.7 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m83.2/83.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for fst-pso (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for miniful (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.23.5 which is incompatible.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.10.1 which is incompatible.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "jaxlib 0.5.1 requires scipy>=1.11.1, but you have scipy 1.10.1 which is incompatible.\n",
            "chex 0.1.89 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "scikit-image 0.25.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "scikit-image 0.25.2 requires scipy>=1.11.4, but you have scipy 1.10.1 which is incompatible.\n",
            "pymc 5.22.0 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\n",
            "bigframes 2.4.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "cvxpy 1.6.5 requires scipy>=1.11.0, but you have scipy 1.10.1 which is incompatible.\n",
            "albucore 0.0.24 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "blosc2 3.3.3 requires numpy>=1.26, but you have numpy 1.23.5 which is incompatible.\n",
            "xarray 2025.3.1 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.23.5 which is incompatible.\n",
            "db-dtypes 1.4.3 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "albumentations 2.0.7 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.23.5 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.23.5 which is incompatible.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "jax 0.5.2 requires scipy>=1.11.1, but you have scipy 1.10.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installer nltk"
      ],
      "metadata": {
        "id": "LReRvVwWvhRX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importer les bibliothÃ¨ques"
      ],
      "metadata": {
        "id": "xDh4IbqvxKZl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZoWNupJxJaA",
        "outputId": "8c3c62c5-3644-40c3-92cf-a6773e493989"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.10.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: FuzzyTM>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (2.0.9)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from FuzzyTM>=0.4.0->gensim) (2.2.3)\n",
            "Requirement already satisfied: pyfume in /usr/local/lib/python3.11/dist-packages (from FuzzyTM>=0.4.0->gensim) (0.3.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2025.2)\n",
            "Requirement already satisfied: simpful in /usr/local/lib/python3.11/dist-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (2.12.0)\n",
            "Requirement already satisfied: fst-pso in /usr/local/lib/python3.11/dist-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (1.8.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (4.13.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->FuzzyTM>=0.4.0->gensim) (1.17.0)\n",
            "Requirement already satisfied: miniful in /usr/local/lib/python3.11/dist-packages (from fst-pso->pyfume->FuzzyTM>=0.4.0->gensim) (0.0.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import des bibliothÃ¨ques de base\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "from time import time"
      ],
      "metadata": {
        "id": "nBaseKbsvf_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Charger mon drive dans google collab"
      ],
      "metadata": {
        "id": "2ex2XoTL5Mdd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmTnk2hs5VIe",
        "outputId": "7449ce4c-0324-4fde-ffae-421cb4b081c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "chargement des DonnÃ©es"
      ],
      "metadata": {
        "id": "obUOO22Y5awe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chemin vers le dossier de donnÃ©es\n",
        "data_dir = \"/content/drive/MyDrive/Parlamint2018_raw\"\n",
        "\n",
        "# Chargement des textes\n",
        "documents = []\n",
        "labels = []\n",
        "\n",
        "for root, dirs, files in os.walk(data_dir):\n",
        "    for file in files:\n",
        "        if file.endswith(\".txt\"):\n",
        "            with open(os.path.join(root, file), \"r\", encoding=\"utf-8\") as f:\n",
        "                text = f.read().strip()\n",
        "                documents.append(text)\n",
        "                labels.append(os.path.basename(root))  # Utilise le nom du dossier comme label\n",
        "\n",
        "print(f\"âœ… {len(documents)} documents chargÃ©s.\")\n",
        "print(f\"âœ… {len(set(labels))} classes dÃ©tectÃ©es.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2G-1n-OR3EiH",
        "outputId": "7df0426a-454e-4224-a1f9-7ef500c828ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… 362 documents chargÃ©s.\n",
            "âœ… 362 classes dÃ©tectÃ©es.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "AperÃ§u Des DonnÃ©es"
      ],
      "metadata": {
        "id": "NOotOjbb6iWr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# AperÃ§u d'un texte et des labels\n",
        "print(\"ğŸ“„ Exemple de texte (500 premiers caractÃ¨res) :\\n\")\n",
        "print(documents[0][:500])\n",
        "\n",
        "print(\"\\n Labels dÃ©tectÃ©s :\")\n",
        "print(sorted(set(labels))[:10])  # Affiche les 10 premiers labels pour un aperÃ§u\n",
        "print(f\"\\n Nombre total de labels : {len(set(labels))}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxDFVwDQ7jvj",
        "outputId": "a2b17aa8-f446-4ed9-8443-75906d16195d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“„ Exemple de texte (500 premiers caractÃ¨res) :\n",
            "\n",
            "Mes chers collÃ¨gues, avant dâ€™en venir aux questions au Gouvernement, je voudrais faire une petite mise au point. Tous les groupes politiques reprÃ©sentÃ©s dans notre assemblÃ©e se sont joints la semaine derniÃ¨re, dans une dÃ©marche commune, Ã  la marche blanche organisÃ©e mercredi en hommage Ã  Mireille Knoll et en signe de refus de lâ€™antisÃ©mitisme. Nos travaux ont dâ€™ailleurs Ã©tÃ© suspendus pour pouvoir aller Ã  cette manifestation. Je veux ici, en votre nom, comme je lâ€™avais fait publiquement dÃ¨s mercre\n",
            "\n",
            " Labels dÃ©tectÃ©s :\n",
            "['ParlaMint-FR_2018-01-16-O1111', 'ParlaMint-FR_2018-01-16-O1112', 'ParlaMint-FR_2018-01-17-O1113', 'ParlaMint-FR_2018-01-17-O1114', 'ParlaMint-FR_2018-01-18-O1115', 'ParlaMint-FR_2018-01-18-O1116', 'ParlaMint-FR_2018-01-18-O1117', 'ParlaMint-FR_2018-01-22-O1118', 'ParlaMint-FR_2018-01-23-O1119', 'ParlaMint-FR_2018-01-23-O1120']\n",
            "\n",
            " Nombre total de labels : 362\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extraction des textes"
      ],
      "metadata": {
        "id": "dJyk-1jP8E3q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Chemin vers le dossier contenant les fichiers .txt\n",
        "data_dir = \"/content/drive/MyDrive/Parlamint2018_raw\"\n",
        "\n",
        "documents = []\n",
        "labels = []\n",
        "\n",
        "# Parcours des sous-dossiers\n",
        "for root, dirs, files in os.walk(data_dir):\n",
        "    for file in files:\n",
        "        if file.endswith(\".txt\"):\n",
        "            file_path = os.path.join(root, file)\n",
        "            try:\n",
        "                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "                    text = f.read().strip()\n",
        "                    documents.append(str(text))\n",
        "                    labels.append(str(os.path.basename(root)))\n",
        "            except Exception as e:\n",
        "                print(f\"âŒ Erreur avec le fichier {file_path} : {e}\")\n",
        "\n",
        "# VÃ©rification\n",
        "print(f\"âœ… Nombre de documents : {len(documents)}\")\n",
        "print(f\"âœ… Nombre de labels : {len(labels)}\")\n",
        "\n",
        "print(f\"Type des Ã©lÃ©ments de documents : {[type(doc) for doc in documents[:5]]}\")\n",
        "print(f\"Type des Ã©lÃ©ments de labels : {[type(label) for label in labels[:5]]}\")\n",
        "\n",
        "\n",
        "#Forcer la convertion en string\n",
        "documents = [str(doc) for doc in documents]\n",
        "labels = [str(label) for label in labels]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0LtvQUZ8GBC",
        "outputId": "8e30ca50-9c36-442f-9fd8-b0cac2e139ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Nombre de documents : 362\n",
            "âœ… Nombre de labels : 362\n",
            "Type des Ã©lÃ©ments de documents : [<class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]\n",
            "Type des Ã©lÃ©ments de labels : [<class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "VÃ©rification"
      ],
      "metadata": {
        "id": "ttJvLvcuE1Iq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, doc in enumerate(documents):\n",
        "    if not isinstance(doc, str):\n",
        "        print(f\"âŒ ProblÃ¨me dans documents Ã  l'indice {i}: {type(doc)}\")\n",
        "\n",
        "for i, lab in enumerate(labels):\n",
        "    if not isinstance(lab, str):\n",
        "        print(f\"âŒ ProblÃ¨me dans labels Ã  l'indice {i}: {type(lab)}\")\n"
      ],
      "metadata": {
        "id": "ZzRb94kFE5Uj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CrÃ©er le dataframe"
      ],
      "metadata": {
        "id": "up-XJ25iDQgB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Nettoyage explicite : filtrer tout ce qui n'est pas une string\n",
        "documents_clean = [str(doc) if not isinstance(doc, str) else doc for doc in documents]\n",
        "labels_clean = [str(label) if not isinstance(label, str) else label for label in labels]\n",
        "\n",
        "# Conversion explicite en listes standard (pour Ã©viter les objets NumPy)\n",
        "documents_clean = list(documents_clean)\n",
        "labels_clean = list(labels_clean)\n",
        "\n",
        "# CrÃ©ation du DataFrame\n",
        "df_texts = pd.DataFrame({\n",
        "    \"label\": pd.Series(labels_clean, dtype=\"string\"),\n",
        "    \"text\": pd.Series(documents_clean, dtype=\"string\")\n",
        "})\n",
        "\n",
        "print(\"âœ… DataFrame crÃ©Ã© avec succÃ¨s.\")\n",
        "display(df_texts.sample(3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "AMpRzJ4UDTNQ",
        "outputId": "d5c78d85-6c73-4eba-caf0-e9ef6bec0735"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Cannot convert numpy.ndarray to numpy.ndarray",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-77b580e634e6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# CrÃ©ation du DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m df_texts = pd.DataFrame({\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;34m\"label\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_clean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"string\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;34m\"text\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments_clean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"string\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmanager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    779\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    478\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdefault_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m         \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_iterable_to_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, data, dtype, copy, name, tupleize_cols)\u001b[0m\n\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m             \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msanitize_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    566\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"index must be specified when data is not list-like\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/construction.py\u001b[0m in \u001b[0;36msanitize_array\u001b[0;34m(data, index, dtype, copy, allow_2d)\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 654\u001b[0;31m             \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_convert_platform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    655\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msubarr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m                 \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/dtypes/cast.py\u001b[0m in \u001b[0;36mmaybe_convert_platform\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_dtype_obj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_convert_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mlib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.maybe_convert_objects\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Cannot convert numpy.ndarray to numpy.ndarray"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installer le modÃ¨le Spacy FranÃ§ais"
      ],
      "metadata": {
        "id": "uz2_hB5xAwh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installer Spacy FranÃ§ais\n",
        "!python -m spacy download fr_core_news_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1iz40X9A708",
        "outputId": "2d9d4b61-c67c-4953-a76c-e1bf28f78181"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fr-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.8.0/fr_core_news_sm-3.8.0-py3-none-any.whl (16.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m88.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fr-core-news-sm\n",
            "Successfully installed fr-core-news-sm-3.8.0\n",
            "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('fr_core_news_sm')\n",
            "\u001b[38;5;3mâš  Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installer & Charger SpaCy"
      ],
      "metadata": {
        "id": "hrc52-0A-qag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import spacy\n",
        "\n",
        "# Charger le modÃ¨le SpaCy pour le franÃ§ais\n",
        "nlp = spacy.load(\"fr_core_news_sm\", disable=[\"ner\", \"parser\"])"
      ],
      "metadata": {
        "id": "Nt5ppb3o-sOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nettoyage des Textes"
      ],
      "metadata": {
        "id": "Lmznr4S1BaXY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def nettoyage_texte(texte):\n",
        "    # Nettoyage de base (garde seulement les lettres et apostrophes)\n",
        "    texte = re.sub(r\"[^a-zA-ZÃ€-Ã¿']+\", ' ', texte)\n",
        "    # Lemmatisation et suppression des stopwords\n",
        "    doc = nlp(texte)\n",
        "    return [token.lemma_ for token in doc if not token.is_stop and len(token) > 2]\n",
        "\n",
        "# Application du nettoyage sur les textes\n",
        "df_texts[\"tokens\"] = df_texts[\"text\"].apply(nettoyage_texte)\n",
        "\n",
        "print(f\"âœ… {len(df_texts)} documents nettoyÃ©s.\")\n",
        "print(f\"ğŸ“ Exemple de tokens : {df_texts['tokens'].iloc[0][:20]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "0yj-vUNcBbFt",
        "outputId": "d48460bc-6ac3-4169-f864-2d1b1f218e96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df_texts' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-efeb54220a2a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Application du nettoyage sur les textes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdf_texts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tokens\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_texts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnettoyage_texte\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"âœ… {len(df_texts)} documents nettoyÃ©s.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_texts' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sauvegarder les tockens nÃ©ttoyÃ©s dans un csv"
      ],
      "metadata": {
        "id": "qTk8GhkINOlK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sauvegarde des tokens nettoyÃ©s dans un fichier CSV\n",
        "df_texts.to_csv(\"documents_nettoyes.csv\", index=False)\n",
        "\n",
        "print(\"âœ… Fichier 'documents_nettoyes.csv' sauvegardÃ©.\")\n"
      ],
      "metadata": {
        "id": "22vbC9aYNZON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chargement du CSV"
      ],
      "metadata": {
        "id": "qYc48DgYPMp-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "# Charger le fichier CSV\n",
        "df_texts = pd.read_csv(\"documents_nettoyes.csv\")\n",
        "\n",
        "# Convertir la colonne \"tokens\" en liste Python\n",
        "df_texts[\"tokens\"] = df_texts[\"tokens\"].apply(ast.literal_eval)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "y6vVP_XPPPUD",
        "outputId": "e61a82ba-ffc2-4f16-8075-160299723db9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'documents_nettoyes.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-193af13c370c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Charger le fichier CSV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"documents_nettoyes.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Convertir la colonne \"tokens\" en liste Python\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'documents_nettoyes.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CrÃ©ation des bigrammes"
      ],
      "metadata": {
        "id": "mQwWD3UMQEtA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Phrases\n",
        "from gensim.models.phrases import Phraser\n",
        "\n",
        "# DÃ©tection des bigrammes\n",
        "phrases = Phrases(df_texts[\"tokens\"], min_count=5, threshold=10)\n",
        "bigram = Phraser(phrases)\n",
        "\n",
        "# Transformation des documents avec les bigrammes\n",
        "df_texts[\"tokens_bigrams\"] = df_texts[\"tokens\"].apply(lambda x: bigram[x])\n",
        "\n",
        "print(f\"âœ… Bigrams crÃ©Ã©s. Exemple : {df_texts['tokens_bigrams'].iloc[0][:20]}\")\n"
      ],
      "metadata": {
        "id": "JoBQYmt8QJd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entrainement du modÃ¨le"
      ],
      "metadata": {
        "id": "EbnChHEYUsKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import multiprocessing\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# PrÃ©paration des phrases pour l'entraÃ®nement\n",
        "sentences = df_texts[\"tokens_bigrams\"].tolist()\n",
        "\n",
        "\n",
        "# ParamÃ¨tres du modÃ¨le\n",
        "w2v_model = Word2Vec(\n",
        "    sentences,\n",
        "    vector_size=100,  # Taille des vecteurs\n",
        "    window=5,  # Contexte\n",
        "    min_count=5,  # FrÃ©quence minimale\n",
        "    workers=multiprocessing.cpu_count(),  # Utilise tous les cÅ“urs disponibles\n",
        "    sg=1  # 1 pour skip-gram (meilleure prÃ©cision), 0 pour CBOW (plus rapide)\n",
        ")\n",
        "\n",
        "# EntraÃ®ner le modÃ¨le\n",
        "w2v_model.train(sentences, total_examples=len(sentences), epochs=10)\n",
        "\n",
        "print(\"âœ… ModÃ¨le Word2Vec entraÃ®nÃ©.\")\n"
      ],
      "metadata": {
        "id": "zgzBM8w_UtMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "VÃ©rifier la similaritÃ©"
      ],
      "metadata": {
        "id": "fFe1AAvhfDNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemples de mots courants dans le corpus\n",
        "exemples = [\"gouvernement\", \"prÃ©sident\", \"ministre\", \"dÃ©mocratie\", \"loi\"]\n",
        "\n",
        "for mot in exemples:\n",
        "    try:\n",
        "        similaires = w2v_model.wv.most_similar(positive=[mot], topn=10)\n",
        "        print(f\"\\nğŸ”¹ Mots les plus similaires Ã  '{mot}' :\")\n",
        "        for sim_mot, score in similaires:\n",
        "            print(f\"   {sim_mot} (similaritÃ© : {score:.2f})\")\n",
        "    except KeyError:\n",
        "        print(f\"âš ï¸ Le mot '{mot}' n'est pas dans le vocabulaire du modÃ¨le.\")\n"
      ],
      "metadata": {
        "id": "dWWNpdnvfGDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculer la similaritÃ© entre deux mots \" gouvernement\" et \"ministre\""
      ],
      "metadata": {
        "id": "ewRBDCHPfprA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# VÃ©rifie la similaritÃ© entre deux mots\n",
        "mot1 = \"gouvernement\"\n",
        "mot2 = \"ministre\"\n",
        "\n",
        "try:\n",
        "    similarite = w2v_model.wv.similarity(mot1, mot2)\n",
        "    print(f\"\\nğŸ”¹ SimilaritÃ© entre '{mot1}' et '{mot2}' : {similarite:.2f}\")\n",
        "except KeyError as e:\n",
        "    print(f\"âš ï¸ {e}\")\n"
      ],
      "metadata": {
        "id": "4UQ18SkpfsWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Clustering***:\n",
        "\n",
        "1.   PrÃ©parer les vecteurs Word2Vec Pour WARD\n",
        "\n"
      ],
      "metadata": {
        "id": "43fOETpKg0L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# RÃ©cupÃ©rer tous les mots du modÃ¨le\n",
        "words = list(w2v_model.wv.index_to_key)\n",
        "\n",
        "# Extraire leurs vecteurs\n",
        "X = np.array([w2v_model.wv[word] for word in words])\n",
        "\n",
        "print(f\"âœ… {len(words)} mots extraits pour le clustering.\")"
      ],
      "metadata": {
        "id": "M8uJl4tCg4IZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(words[:5])\n"
      ],
      "metadata": {
        "id": "z2HKHP_rjz4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "2.   RÃ©duction de dimension( obligatoire pour WARD)\n",
        "L'algorithme de Ward nÃ©cessite des donnÃ©es en espace euclidien, donc il faut rÃ©duire la dimension avec PCA avant de l'utiliser.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "o3PrtEFmriLh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# RÃ©duction de la dimension Ã  20 pour limiter l'utilisation de la mÃ©moire\n",
        "pca = PCA(n_components=20, random_state=42)\n",
        "X_reduced = pca.fit_transform(X)\n",
        "\n",
        "print(f\"âœ… Dimensions rÃ©duites Ã  : {X_reduced.shape}\")\n"
      ],
      "metadata": {
        "id": "4449A0CGrzXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Ã‰tape 1 : PCA vers 10 dimensions (rapide)\n",
        "X_pca_10 = PCA(n_components=10, random_state=42).fit_transform(X_reduced)\n",
        "\n",
        "# Ã‰tape 2 : t-SNE 2D (plus rapide aprÃ¨s rÃ©duction)\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=30, max_iter=1000)\n",
        "X_2d = tsne.fit_transform(X_pca_10)\n",
        "\n",
        "print(f\"âœ… RÃ©duction t-SNE terminÃ©e. Dimensions : {X_2d.shape}\")\n"
      ],
      "metadata": {
        "id": "YI6BKjZe40G3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Clustering avec WARD\n"
      ],
      "metadata": {
        "id": "GQevo8Avv1LZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "n_clusters = 10\n",
        "ward = AgglomerativeClustering(n_clusters=n_clusters, linkage=\"ward\")\n",
        "labels = ward.fit_predict(X_reduced)\n",
        "\n",
        "print(f\"âœ… Clustering terminÃ© avec {n_clusters} clusters.\")"
      ],
      "metadata": {
        "id": "GNIO9HhpyaK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualisation"
      ],
      "metadata": {
        "id": "H_55P_ipxy1F"
      }
    }
  ]
}