# 1. Installer les packages nécessaires (à faire une fois)
install.packages(c("wordspace", "stopwords"), dependencies = TRUE)

# 2. Charger les bibliothèques
library(wordspace)
library(stopwords)

# 3. Spécifier le chemin vers le dossier contenant les fichiers ParlaMint
data_dir <- "/home/miya/Parlamint2018_raw"  # À adapter si besoin

# 4. Lister tous les fichiers .txt dans les sous-dossiers
text_files <- list.files(path = data_dir, pattern = "\\.txt$", recursive = TRUE, full.names = TRUE)

# 5. Lire les contenus des fichiers texte
all_texts <- lapply(text_files, function(f) {
  tryCatch(readLines(f, warn = FALSE, encoding = "UTF-8"), error = function(e) NULL)
})
all_texts <- unlist(all_texts)
all_texts <- all_texts[nzchar(all_texts)]  # enlever les lignes vides

# 6. Fonction de tokenisation ligne par ligne
clean_tokenize_line <- function(line) {
  line <- tolower(line)
  line <- gsub("[^a-zàâçéèêëîïôûùüÿœ\\s]", " ", line)  # remplacer tout sauf lettres/espaces par un espace
  words <- unlist(strsplit(line, "\\s+"))  # découper aux espaces
  words <- words[nchar(words) > 2]  # garder les mots de plus de 2 lettres
  words <- words[!words %in% stopwords("fr")]  # enlever les mots vides
  return(words)
}

# 7. Appliquer la fonction à un sous-ensemble de lignes
max_lines <- 20000
selected_lines <- head(all_texts, max_lines)
tokens_list <- lapply(selected_lines, clean_tokenize_line)
tokens <- unlist(tokens_list)


# C'EST TROP LONG A EXECUTER

# 8. Limiter à 10 000 tokens pour tester rapidement
tokens <- tokens[1:min(10000, length(tokens))]
cat("Nombre de tokens utilisés :", length(tokens), "\n")

# 9. Générer les triplets (mot, contexte, 1)                                  # PAGE 18    essayer different types of co-occurrence
window_size <- 2                                                              # CHANGER CE PARAMETRE ET REGARDER QUOI CA DONNE
triplets <- vector("list", length(tokens) * window_size * 2)
idx <- 1
for (i in seq_along(tokens)) {
  target <- tokens[i]
  context_ids <- (i - window_size):(i + window_size)
  context_ids <- context_ids[context_ids != i & context_ids > 0 & context_ids <= length(tokens)]
  for (c in tokens[context_ids]) {
    triplets[[idx]] <- c(target, c, 1)
    idx <- idx + 1
  }
}
triplets <- triplets[1:(idx-1)]  # retirer les cases vides restantes

# 10. Convertir en table et agréger les fréquences
cooc_df <- as.data.frame(do.call(rbind, triplets), stringsAsFactors = FALSE)
colnames(cooc_df) <- c("term", "context", "score")
cooc_df$score <- as.numeric(cooc_df$score)
cooc_df_agg <- aggregate(score ~ term + context, data = cooc_df, sum)
colnames(cooc_df_agg) <- c("term", "context", "score")  # sécurité

# 11. Créer la matrice DSM brute
dsm_raw <- dsm(cooc_df_agg, raw.freq = TRUE)

# ERREUR
        #Error in dsm.is.canonical(M) :
            #  first argument must be a dense or sparse numeric matrix

#DONC
library(Matrix)

# Vocabulaire commun termes + contextes
all_words <- sort(unique(c(cooc_df_agg$term, cooc_df_agg$context)))

# Indices dans ce vocabulaire
term_idx <- match(cooc_df_agg$term, all_words)
context_idx <- match(cooc_df_agg$context, all_words)

# Matrice creuse
m <- sparseMatrix(
  i = term_idx,
  j = context_idx,
  x = cooc_df_agg$score,
  dims = c(length(all_words), length(all_words)),
  dimnames = list(all_words, all_words)
)

# Créer le DSM                                                 PAGE 74
dsm_raw <- dsm(m, raw.freq = TRUE)


# 12. Appliquer le score d'association PPMI (Positive PMI)     PAGE 38
dsm_ppmi <- dsm.score(dsm_raw, score = "MI")

# 13. Explorer les mots proches de "démocratie"
print(nearest.neighbours(dsm_ppmi, "démocratie", n = 10))     # peut-on clusteriser a la base de cela ?  oui    PAGE 39 DE LA PARTIE 3

# 14. Calculer les distances cosines                           PAGE 49
dist_mat <- dist.matrix(dsm_ppmi, method = "cosine")

# 15. Réduction dimensionnelle (MDS classique)
coords <- cmdscale(dist_mat, k = 2)                           # adapter    #dans wordspace trouver quoi definit la contribution

# 16. Visualisation en 2D
plot(coords, type = "n", main = "Projection DSM (PPMI + cosine)", xlab = "Dim 1", ylab = "Dim 2")
text(coords, labels = rownames(coords), cex = 0.7, col = "blue")

# comment le faire plus lisible ?
# limiter aux mots les plus "contributives"
# factoextra library


# 17. pour clusteriser
eval.clustering(dsm_raw)

#ERROR
        #Error in eval.clustering(dsm_raw) :
            #gold standard does not have a column labelled 'word'

# 18.

gold <- data.frame(
    term = c("démocratie", "république", "élections",
             "marché", "entreprise", "capital"),
    cluster = c("politique", "politique", "politique",
                "économie", "économie", "économie"),
    stringsAsFactors = FALSE
)

eval.clustering(dsm_ppmi, gold)

colnames(gold)[colnames(gold) == "word"] <- "term"

# Sélection des top mots (repris des étapes précédentes)
mat_ppmi <- dsm_ppmi$M
row_sums <- rowSums(mat_ppmi)
top_n <- 100
top_words <- names(sort(row_sums, decreasing = TRUE))[1:top_n]

# Sous-matrice
mat_top <- as.matrix(mat_ppmi[top_words, , drop = FALSE])

# Nettoyage des colonnes constantes
keep_cols <- apply(mat_top, 2, function(col) sd(col) > 0)
mat_top_clean <- mat_top[, keep_cols]

# Matrice DSM avec top mots
dsm_top <- subset(dsm_ppmi, subset = rownames(dsm_ppmi) %in% top_words)

# Matrice de distances cosinus
dist_mat_top <- dist.matrix(dsm_top, method = "cosine")

# Réduction MDS
coords_kmeans <- cmdscale(dist_mat_top, k = 2)

# Visualisation simple
plot(coords_kmeans, type = "n", main = "MDS des mots")
text(coords_kmeans, labels = rownames(coords_kmeans), col = "blue")

# 19. Visualiser MDS + K-means
coords_df <- as.data.frame(coords_kmeans)
colnames(coords_df) <- c("Dim1", "Dim2")  # noms explicites

# K-means clustering sur la nouvelle data.frame
set.seed(123)
k <- 4
clustering <- kmeans(coords_df, centers = k)

library(factoextra)
fviz_cluster(clustering, data = coords_df,
              geom = "text",
              labelsize = 8,
              repel = TRUE,
              main = "Clustering des mots (MDS + K-means)")

# 20. Visualiser PCA + K-means

# Use PPMI-weighted DSM
mat <- as.matrix(dsm_ppmi)

# Filter out rows with zero variance (to avoid PCA error)
non_constant_rows <- apply(mat, 1, function(x) sd(x) > 0)
mat <- mat[non_constant_rows, ]

# (Optional) Keep only top N rows by row sum (e.g. most frequent or "important")
row_scores <- rowSums(mat)
top_n <- 100
top_words <- names(sort(row_scores, decreasing = TRUE))[1:top_n]
mat_top <- mat[top_words, ]

coords_pca <- res.pca$x[, 1:2]  # Take Dim 1 and Dim 2
rownames(coords_pca) <- rownames(mat_top)

# Determine number of clusters (e.g. 4)
k <- 4
set.seed(42)
clustering <- kmeans(coords_pca, centers = k, nstart = 25)

# Visualize with factoextra
library(factoextra)
fviz_cluster(clustering, data = coords_pca,
             geom = "text", labelsize = 8, repel = TRUE,
             main = "Clustering des mots (PCA + K-means)")

