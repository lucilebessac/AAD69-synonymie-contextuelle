<<<<<<< HEAD
import stanza

# phrase = "Quand ils arrivÃ¨rent, le Roi et la Reine Ã©taient assis sur des trÃ´nes. Une grande foule les entourait, composÃ©e de toutes sortes de quadrupÃ¨des et d'oiseaux, et de tout le paquet de cartes. Le Valet de Coeur, debout devant eux, Ã©tait enchaÃ®nÃ© et encadrÃ© de deux soldats. PrÃ¨s du Roi se tenait le Lapin Blanc : il avait une trompette dans une main et dans l'autre un rouleu de parchemin. Sur une table au milieu de la salle Ã©tait posÃ© un grand plateau de tartes. Elles Ã©taient si appÃ©tissantes qu'Alice eut une envie folle de les manger."
# phrase = "Pierre a un chien et Marie a un chat."
phrase = "Le bois est plus lÃ©ger que l'eau puisqu'il remonte Ã  la surface lorsqu'on l'enfonce dans l'eau."
# phrase = "Le roi et la reine arrivent sur le trÃ´ne"


### Ã‰tape 1 : Stanza

# Initialisation de Stanza
nlp = stanza.Pipeline(lang='fr', processors='tokenize,mwt,pos,lemma,depparse', download_method=None)

# DÃ©clencheurs de rel phi et delta
declencheurs_phi = ["csubj", "ccomp", "xcomp","mark", "acl"] # Liste des dep qui dÃ©clenchent une relation phi
    # On exclut advcl pour l'instant car on ne sait pas le gÃ©rer
declencheurs_delta = ["nmod", "appos", "nummod", "amod"] # Liste des dep qui dÃ©clenchent une relation delta

# Traitement de la phrase avec Stanza
doc = nlp(phrase)

liste_enonces = []
enonce_courant = []

for i, sentence in enumerate(doc.sentences):
    print(f"\n========== Phrase {i+1} ==========\n")
    for word in sentence.words:

        # Afficher le mot, la relation de dÃ©pendance et le mot parent
        head_text = sentence.words[word.head - 1].text if word.head > 0 else "ROOT"
        print(f"{word.text} --> {word.deprel} --> {head_text}")
        
        enonce_courant.append(word.text) # On ajoute le mot courant

        ## On segmente si on trouve une rel phi
        if word.deprel in declencheurs_phi:
            print("----- relation phi dÃ©tectÃ©e -----")
            liste_enonces.append(" ".join(enonce_courant)) # On ajoute le courant Ã  la liste complÃ¨te
            enonce_courant = [] #On rÃ©initialise

        # elif word.deprel in declencheurs_delta:
        #     print("----- relation delta dÃ©tectÃ©e -----")
        # else:
        #     None

    # Pour le dernier Ã©noncÃ© qu'il reste
    if enonce_courant:
        liste_enonces.append(" ".join(enonce_courant))
        enonce_courant = []

print("\n========== Liste des Ã©noncÃ©s ==========\n")
for i, enonce in enumerate(liste_enonces, 1):
    print(f"E{i}: {enonce}")
=======
import torch
import torch.serialization
from numpy.core.multiarray import _reconstruct
from spacy_conll import init_parser
import pandas as pd

def patch_torch_load():
    """
    Patch la fonction torch.load pour Ã©viter les erreurs de chargement de modÃ¨le.
    Je ne me souviens pas prÃ©cisÃ©ment quelle est l'errzeur mais la fonction de 
    gÃ©nÃ©ration du conll nÃ©cessite de patcher torch.load pour Ã©viter les erreurs. 
    Je ne sais pas si c'est liÃ© Ã  la version de torch ou Ã  un autre problÃ¨me.
    En gros, il faut patcher torch.load pour qu'il ne lÃ¨ve pas d'erreur lors du
    chargement du modÃ¨le.
    :return: None  
    """
    torch.serialization.add_safe_globals([_reconstruct])

    # ProblÃ¨me sur la fonction torch.load / weights_only=False
    original_torch_load = torch.load

    def patched_torch_load(*args, **kwargs):
        if 'weights_only' not in kwargs:
            kwargs['weights_only'] = False
        return original_torch_load(*args, **kwargs)
    
    torch.load = patched_torch_load

def generate_conll(text, filename):
    """
    GÃ©nÃ¨re un fichier CoNLL Ã  partir d'un texte donnÃ© en utilisant Stanza.
    :param text: Texte Ã  analyser
    :param filename: Nom du fichier de sortie (sans extension)
    """
    nlp = init_parser("fr",
                    "stanza",
                    parser_opts={"use_gpu": True, "verbose": False},
                    include_headers=True)
    doc = nlp(text)

    # sauvegarder au format .conll
    conll = doc._.conll_str
    with open(f"{filename}.conll", "w", encoding="utf-8") as f:
        f.write(conll)

    conll_df = doc._.conll_pd
    conll_df.to_csv(f"{filename}.csv", sep="\t", index=False)


def extraire_heads_et_core_arguments(fichier_csv):
    """
    Extrait les prÃ©dicats principaux et leurs core arguments Ã  partir d'un fichier CoNLL CSV.
    Chaque entrÃ©e aura un ID unique et contiendra la forme du HEAD.

    :param fichier_csv: Chemin du fichier CSV
    :return: Liste des prÃ©dicats avec leurs core arguments sous forme de dictionnaire
    """
    # Charger le fichier CSV
    df = pd.read_csv(fichier_csv, sep=None, engine='python')  # Auto-dÃ©tecter le sÃ©parateur

    # VÃ©rifier si les colonnes nÃ©cessaires sont prÃ©sentes
    if not {"HEAD", "DEPREL", "UPOS", "FORM", "ID"}.issubset(df.columns):
        print("Erreur : Colonnes nÃ©cessaires ('HEAD', 'DEPREL', 'UPOS', 'FORM', 'ID') manquantes.")
        return []

    # Ã‰tape 1 : Identification des prÃ©dicats principaux
    # Liste des conditions pour identifier les prÃ©dicats principaux
    conditions_heads_predicats = (
        (df["UPOS"] == "VERB") |  # Tous les VERB doivent Ãªtre pris
        ((df["UPOS"].isin(["ADJ", "NOUN"])) & (df["DEPREL"].isin(["root", "conj", "ccomp", "xcomp", "advcl", "acl:relcl"])))  # Adjectifs/Noms avec copule
    )

    # Liste des ID qui sont des prÃ©dicats principaux
    # VÃ©rification des conditions
    heads_predicats = df.loc[conditions_heads_predicats, "ID"].dropna().astype(int).unique().tolist()
    print(heads_predicats)
    
    # Ã‰tape 2 : Extraction des core arguments liÃ©s aux prÃ©dicats
    core_arguments = {}
    id_counter = 1  # Initialisation de l'ID unique

    for _, row in df.iterrows():
        head_id = row["HEAD"]  # NumÃ©ro du gouverneur
        dep = row["DEPREL"]  # Relation syntaxique
        form = row["FORM"]  # Mot correspondant

        # VÃ©rifier si le HEAD fait partie des prÃ©dicats principaux
        if head_id in heads_predicats:
            # RÃ©cupÃ©rer la forme du HEAD en utilisant son ID
            head_form = df.loc[df["ID"] == head_id, "FORM"].values
            head_form = head_form[0] if len(head_form) > 0 else None  # Ã‰viter les erreurs si HEAD est absent

            if head_id not in core_arguments:
                core_arguments[head_id] = {
                    "id": id_counter,  # Associer un ID unique
                    "head": head_form,  # Ajouter la **forme du HEAD** dans le dictionnaire
                    "nsubj": [],
                    "obj": [],
                    "iobj": [],
                    "cop": []
                }
                id_counter += 1  # IncrÃ©menter l'ID

            # Ajouter les arguments selon la relation
            if dep == "nsubj":
                core_arguments[head_id]["nsubj"].append(form)
            elif dep == "obj":
                core_arguments[head_id]["obj"].append(form)
            elif dep == "iobj":
                core_arguments[head_id]["iobj"].append(form)
            elif dep == "cop":
                core_arguments[head_id]["cop"].append(form)
            # Ajouter d'autres relations si nÃ©cessaire

    return heads_predicats, core_arguments

def main():
    # patch_torch_load()

    text = "Le sol est mouillÃ© parce quâ€™il a plu cette nuit."
    chemin = "../data/"
    filename = "pluie"
    filename = chemin + filename
    generate_conll(text, filename)

    print(f"Fichiers {filename}.conll et {filename}.csv gÃ©nÃ©rÃ©s avec succÃ¨s !")

    fichier_csv = f"{filename}.csv"  # Mets ici le bon chemin de ton fichier
    heads_predicats, core_args = extraire_heads_et_core_arguments(fichier_csv)

    # Affichage des rÃ©sultats
    print("\nðŸ”¹ **Liste des HEADs correspondant aux prÃ©dicats principaux :**")
    print(heads_predicats)

    print("\nðŸ”¹ **Core Arguments extraits avec ID unique et FORME du HEAD :**")
    for head, args in core_args.items():
        print(f"HEAD '{args['head']}': {args}")


if __name__ == "__main__":
    main()
>>>>>>> lucile
