# 1. Installer les packages nécessaires (à faire une fois)
install.packages(c("wordspace", "stopwords"), dependencies = TRUE)

# 2. Charger les bibliothèques
library(wordspace)
library(stopwords)

# 3. Spécifier le chemin vers le dossier contenant les fichiers ParlaMint
data_dir <- "/home/miya/Parlamint2018_raw"  # À adapter si besoin

# 4. Lister tous les fichiers .txt dans les sous-dossiers
text_files <- list.files(path = data_dir, pattern = "\\.txt$", recursive = TRUE, full.names = TRUE)

# 5. Lire les contenus des fichiers texte
all_texts <- lapply(text_files, function(f) {
  tryCatch(readLines(f, warn = FALSE, encoding = "UTF-8"), error = function(e) NULL)
})
all_texts <- unlist(all_texts)
all_texts <- all_texts[nzchar(all_texts)]  # enlever les lignes vides

# 6. Fonction de tokenisation ligne par ligne
clean_tokenize_line <- function(line) {
  line <- tolower(line)
  line <- gsub("[^a-zàâçéèêëîïôûùüÿœ\\s]", " ", line)  # remplacer tout sauf lettres/espaces par un espace
  words <- unlist(strsplit(line, "\\s+"))  # découper aux espaces
  words <- words[nchar(words) > 2]  # garder les mots de plus de 2 lettres
  words <- words[!words %in% stopwords("fr")]  # enlever les mots vides
  return(words)
}

# 7. Appliquer la fonction à un sous-ensemble de lignes
max_lines <- 20000
selected_lines <- head(all_texts, max_lines)
tokens_list <- lapply(selected_lines, clean_tokenize_line)
tokens <- unlist(tokens_list)


# C'EST TROP LONG A EXECUTER

# 8. Limiter à 10 000 tokens pour tester rapidement
tokens <- tokens[1:min(10000, length(tokens))]
cat("Nombre de tokens utilisés :", length(tokens), "\n")

# 9. Générer les triplets (mot, contexte, 1)
window_size <- 2
triplets <- vector("list", length(tokens) * window_size * 2)
idx <- 1
for (i in seq_along(tokens)) {
  target <- tokens[i]
  context_ids <- (i - window_size):(i + window_size)
  context_ids <- context_ids[context_ids != i & context_ids > 0 & context_ids <= length(tokens)]
  for (c in tokens[context_ids]) {
    triplets[[idx]] <- c(target, c, 1)
    idx <- idx + 1
  }
}
triplets <- triplets[1:(idx-1)]  # retirer les cases vides restantes

# 10. Convertir en table et agréger les fréquences
cooc_df <- as.data.frame(do.call(rbind, triplets), stringsAsFactors = FALSE)
colnames(cooc_df) <- c("term", "context", "score")
cooc_df$score <- as.numeric(cooc_df$score)
cooc_df_agg <- aggregate(score ~ term + context, data = cooc_df, sum)
colnames(cooc_df_agg) <- c("term", "context", "score")  # sécurité

# 11. Créer la matrice DSM brute
dsm_raw <- dsm(cooc_df_agg, raw.freq = TRUE)

# ERREUR
        #Error in dsm.is.canonical(M) :
            #  first argument must be a dense or sparse numeric matrix

#DONC
library(Matrix)

# Vocabulaire commun termes + contextes
all_words <- sort(unique(c(cooc_df_agg$term, cooc_df_agg$context)))

# Indices dans ce vocabulaire
term_idx <- match(cooc_df_agg$term, all_words)
context_idx <- match(cooc_df_agg$context, all_words)

# Matrice creuse
m <- sparseMatrix(
  i = term_idx,
  j = context_idx,
  x = cooc_df_agg$score,
  dims = c(length(all_words), length(all_words)),
  dimnames = list(all_words, all_words)
)

# Créer le DSM                                                 PAGE 74
dsm_raw <- dsm(m, raw.freq = TRUE)


# 12. Appliquer le score d'association PPMI (Positive PMI)     PAGE 38
dsm_ppmi <- dsm.score(dsm_raw, score = "MI")

# 13. Explorer les mots proches de "démocratie"
print(nearest.neighbours(dsm_ppmi, "démocratie", n = 10))     # peut-on clusteriser a la base de cela ?  oui    PAGE 39 DE LA PARTIE 3

# 14. Calculer les distances cosines                           PAGE 49
dist_mat <- dist.matrix(dsm_ppmi, method = "cosine")

# 15. Réduction dimensionnelle (MDS classique)
coords <- cmdscale(dist_mat, k = 2)                           # adapter    #dans wordspace trouver quoi definit la contribution

# 16. Visualisation en 2D
plot(coords, type = "n", main = "Projection DSM (PPMI + cosine)", xlab = "Dim 1", ylab = "Dim 2")
text(coords, labels = rownames(coords), cex = 0.7, col = "blue")

# comment le faire plus lisible ?
# limiter aux mots les plus "contributives"
# factoextra library


# 17. pour clusteriser
eval.clustering(dsm_raw)

#ERROR
        #Error in eval.clustering(dsm_raw) :
            #gold standard does not have a column labelled 'word'